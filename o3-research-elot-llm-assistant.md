I’ll investigate whether Elot’s literate, outline-based format makes LLM assistants more effective for ontology authoring, especially compared to traditional formats like Turtle or OWL/XML. I’ll also assess Elot’s structure from a technical standpoint to identify features that help or hinder LLMs, and I’ll include practical guidance on how to format Elot documents for best results with LLMs. I’ll also cover key use cases like adding axioms, generating definitions, or expanding prose explanations. I’ll let you know when the report is ready.

# ELOT Format and LLM-Assisted Ontology Authoring

**Hypothesis:** *Using the Emacs Literate Ontology Tool (ELOT) – which represents ontologies as Org-mode outlines with interwoven prose, code, and axioms – can make large language models (LLMs) like ChatGPT more effective and productive in assisting ontology authoring, compared to traditional formats (OWL/XML, Turtle, or GUI tools like Protégé).* This analysis examines ELOT’s structure versus conventional ontology formats, and evaluates how ELOT’s human-readable, structured approach might enhance LLM-based editing, augmentation, and documentation of ontologies.

## ELOT’s Literate Ontology Format vs. Traditional Formats

**ELOT (Emacs Literate Ontology Tool):** ELOT lets authors write an OWL ontology as a plain-text Org-mode document, combining *explanatory narrative* with *formal ontology code* ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=ontologies%20in%20a%20literate%20programming,ELOT%E2%80%99s%20capabilities%20%E2%80%93%20including%20editing)). Each ontology element (class, property, etc.) is an outline heading with a human-readable label and a formal identifier. For example, a class might appear as: `*** Mammal (ex:Mammal)` – where **Mammal** is a readable label and `(ex:Mammal)` is the OWL CURIE (prefix and name) ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=ELOT%20treats%20an%20ontology%20as,as)). Hierarchical nesting of headings conveys subclass relationships (nesting *Primate* under *Mammal* means *Primate* is a subclass of *Mammal* ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=headings%2C%20using%20a%20format%20like,as))). Under each heading, **description list** items (`- Key :: Value`) provide details. If the “Key” is an annotation property (e.g. `rdfs:comment` or `skos:definition`), the line is treated as an annotation; if it’s a logical axiom keyword (`SubClassOf`, `EquivalentTo`, etc.), it defines an OWL axiom ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=a%20subclass%20of%20,g)). ELOT uses the OWL Manchester Syntax for axioms, a compact, human-readable notation ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=readable%20and%20concise%20for%20class,ontology%20can%20be%20used%20in)). All formal content can be *“tangled”* out of the Org file into a machine-readable OWL file (in Manchester or converted to Turtle) while the narrative and headings can be exported as a nicely formatted documentation (HTML or PDF) ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=ontologies%20in%20a%20literate%20programming,ELOT%E2%80%99s%20capabilities%20%E2%80%93%20including%20editing)). In essence, ELOT is a single-source that produces both the OWL ontology and its documentation, in sync.

**Traditional Formats:** By contrast, ontologies are often edited either in *raw serialization formats* (like OWL/XML, RDF/XML, or Turtle) or via *GUI tools* like Protégé. OWL/XML and RDF/XML are verbose XML-based formats, primarily machine-oriented. Turtle (TTL) is a more readable text serialization of RDF triples, but it still separates human commentary from logic – one typically relies on rdfs:comment or skos:definition triples for documentation. These formats lack a built-in narrative structure: they list axioms/triples in a flat text, requiring external context to interpret groupings or hierarchy (e.g. subclass relations appear as scattered triples). **Protégé** and similar tools provide a user-friendly interface (class trees, forms for properties, etc.), but any extensive documentation (beyond short comments) is not part of the ontology file itself. Documentation in traditional workflows is often generated separately (for example, using tools like WIDOCO to produce HTML pages from OWL annotations) ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=Comparatively%2C%20most%20other%20ontology%20tools,coherent%20commentary%20around%20the%20ontology)). This means the *explanatory text is siloed* away from the ontology source. Protégé allows adding annotations (labels, comments) for each term, but it doesn’t produce a cohesive narrative document; users must use external docs or interfaces to read those annotations ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=Comparatively%2C%20most%20other%20ontology%20tools,coherent%20commentary%20around%20the%20ontology)).

**Key Structural Differences:**

- *Human-Readability:* ELOT treats the ontology as a *human-readable document first*, with formal definitions embedded ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=ontologies%20in%20a%20literate%20programming,ELOT%E2%80%99s%20capabilities%20%E2%80%93%20including%20editing)). Traditional formats treat the ontology as a data file, with human readability as an afterthought (often requiring separate documentation). ELOT’s output is akin to a well-commented outline or a textbook of the ontology, whereas a Turtle or OWL/XML file is more like a database dump.
- *Outline Hierarchy:* In ELOT, the hierarchy is explicit in the outline structure (headings and subheadings), making parent-child relationships immediately visible in context. In Turtle or OWL/XML, hierarchy must be inferred by finding all `rdfs:subClassOf` triples – logically equivalent, but not as easy to **see** in one place. A GUI like Protégé visualizes the class tree, but if one were to feed the raw file to an LLM, that structural clarity is not inherently obvious in the text.
- *Integrated Documentation:* ELOT allows free-form *prose sections* anywhere in the document (e.g. an “About” section or guidance notes) that do not become OWL content ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=Because%20ELOT%20is%20built%20around,introduction%20or%20a%20user%20guide)). Authors can tag such sections with `:nodeclare:` so they are ignored in the OWL export, existing purely as documentation ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=human,machine)). This means an ontology written in ELOT can include rationales, examples, and editorial notes inline with the formal content. Traditional formats have no mechanism for inline narrative beyond brief comments; extensive explanations would live outside the ontology file ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=Comparatively%2C%20most%20other%20ontology%20tools,coherent%20commentary%20around%20the%20ontology)).
- *Literate Programming Paradigm:* ELOT’s approach is analogous to literate programming or Jupyter notebooks: the *explanation and the code live side by side* ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=markup%20,does%20so%20for%20ontology%20engineering)). The Org-mode file can even include live query blocks (SPARQL) or auto-generated diagrams reflecting the ontology, which update as the ontology changes ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=In%20addition%20to%20static%20text%2C,This%20literate%20programming%20approach%20%E2%80%93)) ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=rdfpuml%20,ELOT%20even)). Conventional ontology files can’t contain executable documentation or diagrams – those must be created separately.

Given these differences, **the hypothesis is that ELOT’s readable, well-structured format provides richer context and clarity that an LLM can leverage**, whereas traditional formats might leave the LLM with sparse context (just identifiers and logical statements).

## How ELOT’s Structure Benefits LLM Parsing & Generation

Large Language Models operate by predicting text based on patterns in their training data. A format that is *closer to natural language and well-structured* will generally be easier for an LLM to understand and manipulate. ELOT’s design seems naturally advantageous in this regard:

- **Clarity of Language:** ELOT expresses axioms in Manchester Syntax (e.g. `hasPart some Heart`) embedded in a narrative style document. Manchester syntax is relatively close to English (using words like “some”, “only”, “and”) and was explicitly designed to be human-readable. An LLM is more likely to have seen and learned from such pseudo-natural language patterns than from raw OWL/XML. In fact, including ontology triples in a prompt *does* help an LLM produce more rigorous results ([Writing An Ontology with ChatGPT3 - X-Lab](https://carleton.ca/xlab/2023/writing-an-ontology-with-chatgpt3/#:~:text=No%20doubt%20this%20is%20not,transcript%20of%20his%20chat%20here)). For example, one experiment found that giving GPT-3 an ontology in Turtle format improved the consistency of information it extracted ([Writing An Ontology with ChatGPT3 - X-Lab](https://carleton.ca/xlab/2023/writing-an-ontology-with-chatgpt3/#:~:text=No%20doubt%20this%20is%20not,transcript%20of%20his%20chat%20here)). ELOT goes a step further by surrounding formal statements with plain English explanations and labels, making the input even more understandable to the model.

- **Structured Context:** The Org-mode outline provides **contextual grouping**. When an LLM reads an ELOT document, the organization itself conveys meaning: under the “**Continuant (obo:BFO_0000002)**” heading in BFO’s ELOT file, it immediately finds that term’s definition, examples, and axioms in one section ([bfo-core.org](file://file-SpJXa1zFf5kAyEnrzRRKQx#:~:text=%2A%2A%2A%2A%20%22continuant%22%40en%20%28obo%3ABFO_0000002%29%20,en%20%28obo%3ABFO_0000031)). It doesn’t have to hunt through a long list of triples to gather what “continuant” means; the information is collated. This structured presentation can improve comprehension. The LLM can treat each heading as a self-contained topic, which aligns with how it processes topical segments in text. In contrast, in a raw Turtle file, the model might need to aggregate data from distinct parts of the file (e.g., find the rdfs:label in one place, the definition annotation elsewhere, the subclass axioms elsewhere). The cognitive load (for the LLM) is higher with the traditional format – essentially akin to piecing together a puzzle versus reading a coherent paragraph.

- **Narrative and Explanations:** Because ELOT encourages authors to include explanatory **narrative sections** and comments inline ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=Because%20ELOT%20is%20built%20around,llm)) ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=prompt,always%20synchronized%20with%20the%20ontology)), an LLM has access to the *why* behind modeling decisions, not just the *what*. This additional natural language context can guide the model’s output. For instance, if a section explains “We introduce class X because ...”, the LLM will understand the intent and could use that to generate more relevant suggestions or explanations. In a traditional ontology file, the model might only see that class X is a subclass of Y with no explanation of its purpose. This transparency likely improves the accuracy of LLM completions or summaries, as the model isn’t guessing the rationale – it’s given in text.

- **Reduced Ambiguity:** ELOT uses explicit labels and curated identifiers. In the BFO example above, the class is shown as `"generically dependent continuant"@en (obo:BFO_0000031)` ([bfo-core.org](file://file-SpJXa1zFf5kAyEnrzRRKQx#:~:text=,file%20that%20is%20a%20copy)). An LLM can tell the human-intended name is “generically dependent continuant” and the technical ID is BFO_0000031. If the LLM were to see only the ID (as would be the case in a raw OWL file with no label), it might not recognize it, or it might have to infer meaning from the ID string alone. A user of ChatGPT noted that *ChatGPT struggled to interpret an OWL ontology when classes lacked labels*, only IDs – the model “could barely parse the ontology” until labels were added ([Using ChatGPT to Generate English Labels for a Spanish Ontology](https://www.michaeldebellis.com/post/generating_english_ontology_labels#:~:text=labels%20in%20English%20as%20well,as%20Spanish)). ELOT inherently provides labels for every entity (the heading text), so the model always has a human-readable handle to latch onto.

- **Consistent Pattern for Axioms:** The `- Key :: Value` list format in ELOT means that every axiom or annotation follows a predictable textual pattern. LLMs excel at learning and following such patterns. Once it sees a few examples (e.g. `- SubClassOf :: ex:Parent and hasRole some ex:Role`), it can generate new ones in the same style. Traditional formats are less homogeneous: some axioms might be Turtle triples, others might be OWL functional syntax if using certain tools, etc. The consistency of ELOT’s markdown-like syntax is a simpler pattern to emulate. Additionally, because ELOT segregates different axiom types by the key (`SubClassOf`, `EquivalentTo`, etc.), the LLM has clear keywords to guide it, rather than having to recall RDF vocabulary like `rdfs:subClassOf` or blank node structures for restrictions. In short, ELOT’s streamlined syntax likely yields more *predictable and correct output* from the LLM when it’s asked to add or modify axioms.

- **Single-Source of Truth:** With ELOT, the model can be given *one document that contains everything* – the formal axioms and the documentation. This avoids the model having to reconcile multiple sources. If we asked an LLM to assist with a Turtle ontology, we might also have to feed it separate documentation or explain context in the prompt, increasing prompt length and complexity. ELOT’s single-source format is directly feedable. As a bonus, the *cohesion* of the document means the LLM can cross-reference within it: e.g., if class A’s description mentions class B, the LLM can scroll to class B’s section to understand it before making a suggestion about class A.

In summary, the **transparency and structure** of ELOT align well with LLMs’ strengths (understanding natural language and following format patterns). The format effectively “pre-processes” the ontology into a more digestible form for the LLM. Conversely, conventional formats often require the LLM to perform that synthesis itself (e.g., mentally mapping cryptic IRIs to concepts, or grouping scattered axioms by subject), which increases the chance of error or omission.

## LLM-Assisted Ontology Authoring: Key Use Cases

Leveraging an LLM like ChatGPT in ontology development can span various assistive tasks. We explore several key use cases and how ELOT’s format might enhance them:

- **Natural Language Explanations of Axioms:** Ontologies often contain complex axioms (restrictions, logical definitions) that are not immediately intuitive. An LLM can translate these into plain English. For example, given an axiom `- EquivalentTo :: ex:Parent and hasChild some ex:Child`, a model could produce a sentence: “*A Parent is defined as an individual that has at least one Child*.” ELOT makes this easier by providing the axiom in a relatively English-like Manchester form and by situating it under the class with that class’s label (“Parent”) visible ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=a%20subclass%20of%20,g)). This means the LLM doesn’t have to guess what `ex:Parent` refers to – it sees the label *Parent* right there. The surrounding context (perhaps a `skos:definition` or a comment for *Parent*) can further guide the phrasing. In a traditional Turtle file, the model might see something like `ex:Parent rdfs:subClassOf ex:Person . ex:Parent rdfs:subClassOf [ rdf:type owl:Restriction ... ]` which is much harder to turn into fluent text. The literate style of ELOT essentially provides built-in scaffolding for explanation. Early experiments show GPT-4 is indeed capable of interpreting OWL expressions and providing reasonable explanations or suggestions ([](https://2024.eswc-conferences.org/wp-content/uploads/2024/04/146640137.pdf#:~:text=Thoughts%20,LLMs%20to%20assist%20ontology%20engineers)), and a format like ELOT would only improve its accuracy by reducing ambiguity. This use case is valuable for documenting the ontology (an LLM could generate or check the prose explanation of a complex logical constraint) and for teaching or knowledge sharing (the LLM could answer “What does this class axiom mean?” using the ELOT doc as reference).

- **Auto-expanding Definitions and Labels:** Creating good textual definitions for ontology classes is a creative task that LLMs are well-suited for. If an ontology term lacks a definition or human-friendly label, an LLM can propose one based on the term name and context. ELOT is advantageous here because each class is presented with a clear name and any existing annotations. The LLM can be prompted with the class’s section (which might include its parent class and any relations) and asked to draft a definition. For instance, suppose we have a class **"Cardiac Valve (ex:HeartValve)"** with no `skos:definition` yet. The LLM can recognize from the label and perhaps its parent class (say, under **Heart Structure**) that it’s a heart valve, and generate a definition: “*A heart valve is a flap of tissue in the heart that regulates blood flow direction.*”. Because ELOT would allow us to slot that text right into the Org file (`- skos:definition :: "...".`), the integration is seamless. In a pure Turtle workflow, one could ask the LLM for a definition, but then the user must manually insert it as an `rdfs:comment` triple or similar. ELOT’s context also helps ensure the definition is *correct and specific*: the LLM can see siblings or examples in the narrative. A study on AI-assisted ontology editing (DRAGON-AI) found that LLM-generated definitions were often acceptable, though typically slightly lower in quality than expert-written ones ([Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI) | Journal of Biomedical Semantics | Full Text](https://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-024-00320-3#:~:text=We%20assessed%20performance%20of%20DRAGON,the%20form%20of%20GitHub%20issues)). Having those suggestions is still a huge productivity boost – the ontology author can refine an AI-proposed definition rather than writing from scratch. Moreover, ELOT’s literate environment could allow storing *the prompt and draft* in the document for traceability. (For example, an author might include a hidden comment: “%% GPT suggested: ...”). This creates a record that wouldn’t be available if using a GUI tool. It’s worth noting that adding labels is something LLMs can do even for other formats (for instance, using ChatGPT to generate English labels for classes that only have Spanish labels ([Using ChatGPT to Generate English Labels for a Spanish Ontology](https://www.michaeldebellis.com/post/generating_english_ontology_labels#:~:text=Today%20I%20had%20the%20problem,found%20here%3A%20ITEMAS%20on%20BioPortal))), but when the ontology is in a friendly format, the LLM can not only propose the text but also place it correctly in context. ELOT inherently uses human-readable labels, avoiding the scenario where the LLM is handicapped by opaque identifiers ([Using ChatGPT to Generate English Labels for a Spanish Ontology](https://www.michaeldebellis.com/post/generating_english_ontology_labels#:~:text=This%20didn%27t%20work,That%27s%20an)).

- **Suggesting Missing Annotations or Relationships:** An LLM can act as a sort of smart assistant checking the ontology for gaps or logical connections. Thanks to its training on vast knowledge (including likely many ontologies and schemas), it might infer that *if* certain classes exist, a particular relationship should also exist. For example, if an ontology has a class “Heart” and a class “CirculatorySystem,” the LLM might suggest: “Should Heart be part of CirculatorySystem? Many knowledge bases link heart as part of the circulatory system.” In ELOT, implementing that suggestion is straightforward: the user (or the LLM itself, if directly editing) would add a list item under **Heart**: `- PartOf :: ex:CirculatorySystem`. Because the model can *see the whole ontology outline*, it can identify such omissions by pattern. Perhaps every **Device** in an ontology has a manufacturer specified except one; an LLM could flag that inconsistency. Or it might observe “Class A and Class B are disjoint in many ontologies I know, but your ontology doesn’t declare them disjoint; consider adding `DisjointWith`.” Indeed, research has demonstrated high precision in LLMs proposing ontology **relationships** (like subclass links or domain-range associations) when given enough background context ([Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI) | Journal of Biomedical Semantics | Full Text](https://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-024-00320-3#:~:text=We%20assessed%20performance%20of%20DRAGON,the%20form%20of%20GitHub%20issues)). ELOT’s format might amplify this by giving the model that background in a digestible form. In a Turtle file, the model could still attempt this, but it’s more likely to miss context or misinterpret due to the scattered nature of information. With ELOT, suggestions for missing axioms can be made in-line. The author can prompt, for example: “*Review class X: suggest any additional relationships or annotations it might need.*” The LLM can then answer with a diff or a list of `- Key :: Value` lines to add. The structured nature means the LLM will likely output the suggestion in the correct format (since it sees how other entries look) rather than just a vague instruction.

- **Guidance and Critical Review of Ontology Structure:** Beyond generating content, LLMs can be used as advisors, spotting potential modeling issues or offering design guidance. ELOT’s readable outline makes it feasible to have the LLM *read and critique* the ontology as a whole (or large sections of it). For instance, an LLM could be asked: “*Analyze the ontology and identify any classes that might be misclassified or any sections that seem inconsistent.*” Because the document is interwoven with commentary, the LLM might catch things like contradictory definitions, overly broad or narrow hierarchies, or missing coverage of a domain. For example, if an author described in narrative that “We model temporal concepts under Continuant,” but the LLM sees a class *TimeInterval* under **Occurrent**, it could flag that discrepancy. Or it might simply point out style inconsistencies (one part of the ontology uses `skos:definition`, another uses `rdfs:comment` for definitions – a human might overlook this, but an attentive LLM could notice the pattern difference). Essentially, the LLM can serve as an *extra pair of eyes*, giving feedback similar to a human reviewer. With Protégé alone, doing this requires the human to manually inspect or to write SPARQL queries to find issues. An LLM can work from the text and its built-in knowledge of ontology best practices. Indeed, researchers have suggested that advanced LLMs like GPT-4 can provide suggestions comparable to what a novice ontology engineer might do ([](https://2024.eswc-conferences.org/wp-content/uploads/2024/04/146640137.pdf#:~:text=method,LLMs%20to%20assist%20ontology%20engineers)) – for example, advising on how to restructure a hierarchy or refine an axiom. ELOT’s format would allow the LLM to refer directly to specific sections by name. The model might respond with: “*I notice that the class **Person** has no defined age property while **Animal** does; if this is an oversight, consider adding an **age** attribute to Person.*” – referencing the actual class names and terms as written in the document. This level of direct reference is possible because the ELOT document’s terms are readable; if it were an OWL/XML, the LLM might instead output something like “add missing property to class with IRI …” which is less useful. One could even imagine the LLM providing a summary “report” of the ontology’s quality (e.g., listing how many classes lack definitions, or identifying orphan classes with no connections) – tasks that are doable with scripting but now can be done with a conversational query to the model, aided by the literate format for easy interpretation.

In all these use cases, ELOT provides a *convenient canvas* for LLM interactions. The model can both **read** and **write** in the same document format as a human author, which opens up interactive workflows: the human and AI can iteratively refine the ontology in the Org document. For instance, the human writes a skeleton of classes, the AI fills in some annotations, the human verifies and asks for corrections, and so on – much like pair programming, but for ontology engineering. This is much harder to do smoothly if the ontology is hidden behind a GUI or locked in a less human-friendly syntax.

## Critical Assessment of the Hypothesis

Overall, the hypothesis that ELOT’s literate, outline-based format improves LLM-assisted ontology development is **well-supported by the nature of the format and preliminary evidence**, though it hasn’t been formally tested in literature yet. The advantages discussed – improved readability, context, and structure – align directly with factors known to help LLMs generate better output. Early case studies and research provide tangential support: for example, prompting GPT models with structured ontology content (like TTL triples) yields better knowledge extraction ([Writing An Ontology with ChatGPT3 - X-Lab](https://carleton.ca/xlab/2023/writing-an-ontology-with-chatgpt3/#:~:text=No%20doubt%20this%20is%20not,transcript%20of%20his%20chat%20here)), and GPT-4 has demonstrated the ability to produce quality OWL ontology suggestions when guided properly ([](https://2024.eswc-conferences.org/wp-content/uploads/2024/04/146640137.pdf#:~:text=Thoughts%20,LLMs%20to%20assist%20ontology%20engineers)). These suggest that when an ontology is presented in a *transparent and organized manner*, LLMs can indeed contribute meaningfully. ELOT is arguably an ideal way to present an ontology to both humans and machines simultaneously.

A critical look does reveal some **caveats** and areas to watch out for:

- **LLM Familiarity with Format:** ELOT’s syntax (Org-mode with custom conventions) is not a standard that an LLM would have seen frequently during training. By contrast, Turtle or even Manchester syntax appears in many texts and documentation (and the OWL API examples). This means that an LLM might need a few examples or careful prompting to fully “grasp” the ELOT conventions (e.g. it might not initially know that `*** Heading (ex:Class)` means a class declaration). In practice, this is easily addressed by providing the model a demonstration of the format (shot examples) or simply by virtue of context – if the entire document is in this style, the model will learn it quickly within the session. Still, it’s a difference: the ELOT format is advantageous *once the LLM is on board with it*, but it requires a tiny onboarding, whereas Turtle is more likely to be understood out-of-the-box. This is a minor issue given LLMs’ adaptability.

- **Potential for Confusion:** The intermixing of narrative text and formal code could, in some cases, confuse an LLM if prompts are not well crafted. For example, if asked to “generate the ontology fragment for X,” the model might include explanatory text from the narrative in its output unintentionally. It’s important to clearly instruct the LLM when we want only the formal part vs. when we welcome narrative. The nicely delineated structure of ELOT (with `#+BEGIN_SRC omn` blocks for code, and separate prose sections) helps here – a good prompt might say “only output the content for the OWL block” and the model can do so. Emacs Org-mode also uses specific drawers (like the `:PROPERTIES:` or `:OMN:` drawer) that ELOT employs; an LLM might not inherently know to leave those alone or fill them properly. Again, careful prompting or possibly minor fine-tuning could mitigate this. It’s a new workflow, so users would need to develop best practices (which we outline in the next section).

- **Context Size Limits:** Literate ontology files can be quite large (imagine a big ontology with hundreds of classes, each documented). Current LLMs have context length limits (e.g. 4k, 8k, or 32k tokens depending on model). There is a risk that an entire ELOT document might exceed that, meaning the model cannot ingest the whole ontology at once. Traditional formats have the same issue (a large Turtle file is also lengthy), but ELOT’s extra commentary makes the file even longer. However, this is only a practical limitation for very large ontologies; and even then, one can work on one section at a time (e.g., focus the LLM on a subset of the ontology). As model context sizes grow, this will become less of a concern. It’s also worth noting that having the extra context (even if not the whole ontology) is usually beneficial – you might not need to feed *everything* to the LLM, just the relevant parts plus some surrounding context. ELOT makes it easy to extract those relevant parts (since you can copy a self-contained section). With a pure OWL file, you might accidentally omit needed prefixes or imports when copying fragments, which could confuse the model. ELOT sections tend to be more self-explanatory.

- **Validation and Accuracy:** While LLMs can suggest axioms or definitions, they can also hallucinate or introduce errors. ELOT’s format doesn’t inherently prevent wrong suggestions – but it does make them **more visible** to the human. For example, if the LLM inserts a new class in the wrong place in the outline, a quick glance will catch that the hierarchy looks off. In a diff of a Turtle file, a subtle change might be missed by a human reviewer. So, ELOT could aid the *human oversight* part of the loop. Nonetheless, one should not assume that just because the LLM output is in a literate format it is correct. Standard verification (running a reasoner, using ontology QA tools) should still be applied. The upside is that ELOT integrates with such tools (via OWL API, ROBOT, etc.) easily ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=readable%20and%20concise%20for%20class,ontology%20can%20be%20used%20in)), so one can round-trip: have LLM propose changes in ELOT, tangle to OWL, run reasoner to check, then update the ELOT doc accordingly. The hypothesis holds that ELOT makes the *collaboration* with the LLM easier, not that it makes ontologies automatically correct.

- **User Expertise and Workflow:** Adopting ELOT requires using Emacs/Org-mode and a literate programming mindset. Not all ontology authors are comfortable in that environment (many are used to Protégé’s GUI). For those users, working with an LLM might currently involve plain language queries or Protégé plugins, rather than editing text. If they were to leverage ELOT purely to interface with an LLM, there’s a learning curve to consider. However, the question is about effectiveness when *assisting* in authoring – so assuming the user is using ELOT, does the LLM help more? If the user is already in Protégé, the LLM could still help (e.g., via natural language to OWL suggestions) but some advantages of the literate format (like inline explanations) wouldn’t be available. In short, the benefit is clear *if one is already inclined towards a text-based ontology workflow*. This is similar to how a programmer might get more out of GitHub Copilot if their code is clean and well-documented – the tool synergy is best when good practices are in place. ELOT enforces many good practices (documentation, organization), which in turn boosts LLM performance. So the critical view would say: *the hypothesis holds true mostly in scenarios where the ontology authoring process is configured to take advantage of it* (i.e., using ELOT with an LLM integration), rather than universally for any user.

In balance, these considerations do not undermine the hypothesis, but they highlight that **human-AI collaboration needs guidance**. ELOT provides an excellent medium, but one must still apply the right prompts and checks. Importantly, nothing about ELOT hinders LLM assistance – the concerns above are manageable. On the flip side, many benefits of ELOT for human comprehension clearly carry over to machine (LLM) comprehension. Given the evidence and reasoning, the hypothesis appears *credible*: ELOT’s literate, transparent format can make ChatGPT-style assistants more effective in ontology editing than they would be on more opaque formats.

## Best Practices for LLM-Friendly ELOT Documents

If one accepts that ELOT and LLMs together are a powerful combination, how should we format and write ELOT documents to maximize this synergy? Here are some concrete guidelines and tips:

- **Provide Rich Annotations:** Ensure each ontology element in the ELOT file has human-readable annotations (labels, definitions, examples) in the description list. The LLM uses these to understand intent. For instance, include a `skos:definition` and maybe a `skos:example` for important classes. As noted, ChatGPT had trouble when labels were missing ([Using ChatGPT to Generate English Labels for a Spanish Ontology](https://www.michaeldebellis.com/post/generating_english_ontology_labels#:~:text=This%20didn%27t%20work,That%27s%20an)) – so never rely on IDs alone. In ELOT, this means always titling headings with a quoted label and filling in at least a one-line description for new classes/properties. Even a short note like `- rdfs:comment :: "Placeholder, needs definition"` is better than nothing (the LLM might even pick up on “needs definition” and attempt to help!).

- **Maintain Consistent Structure:** Adhere to ELOT’s standard outline structure for classes, object properties, etc., using the provided templates (e.g., the `ods` template that sets up sections for Classes, Properties, etc. ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=In%20practice%2C%20ELOT%20provides%20a,in))). Consistency makes it easier for an LLM to navigate and predict where to put things. For example, if all classes are under the “Classes” section and use third-level headings (***), the LLM won’t mistakenly create a class at the wrong outline level. Similarly, always use the `Key :: Value` syntax exactly – models are sensitive to small format changes. ELOT’s tooling (the “hydra” menu and shortcuts) can help insert properly formatted skeletons ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=In%20practice%2C%20ELOT%20provides%20a,in)), so using those not only helps the human avoid syntax errors but also gives the LLM consistent patterns to follow.

- **Use Org Tags for Clarity:** Take advantage of Org-mode tags like `:nodeclare:` (as ELOT supports) to mark purely narrative sections ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=future%20editors,machine)). This isn’t directly for the LLM’s benefit (the LLM will read those sections regardless), but it keeps the ontology generation clean. You can even use a custom tag like `:AI:` on sections that contain AI-generated content or suggestions. This could help you (and potentially an LLM) distinguish which parts were machine-proposed. For instance, you might add a comment, “%% This definition was suggested by GPT-4, needs review. :AI:” – the LLM reading later might even adjust its tone knowing it’s revising its own prior suggestion.

- **Chunk the Work with Sectional Prompts:** If the ontology is large, work section by section. You can copy the text of a single class (or a subset of classes) into the prompt and focus the LLM on that. Because ELOT’s file is naturally divided into sections, it’s easy to select a contiguous block. For example, to have the LLM review all **Disease** subclasses, you could copy the “Disease” subtree. This avoids context overflow and keeps the model’s attention focused. It also mirrors how a human would review an ontology: topic by topic. By structuring the Org document with clear headings and perhaps an index, you facilitate targeted prompts.

- **Leverage Narrative for Prompting:** When asking the LLM for something, consider putting the question or instruction *in the document itself* as a comment or TODO, then feeding that context. E.g., under a class you might write an HTML comment or Org comment `<!-- TODO: Explain why this class is needed -->` or Org’s `#+BEGIN_COMMENT ... #+END_COMMENT` block with a question. While the LLM will see this in the prompt, it won’t end up in the tangled OWL output (comments or non-exported sections won’t tangle). This technique essentially lets you embed “live queries” in your ontology text. The LLM can answer by producing content right after the comment. This keeps the workflow within the document: you’re essentially conversing with the LLM inside your Org file. Just remember to remove or mark resolved such queries after addressing them, to avoid confusion later.

- **Avoid Overloading with Irrelevant Text:** While narrative is great, try to keep the ELOT document focused and up-to-date. Outdated notes or extremely verbose discussions might distract the LLM or lead it down the wrong path. For instance, if an old comment says “We might use approach X (note: not decided)”, the LLM might treat that as still current and give suggestions related to X even if you chose approach Y later. So, curate the content: archive or remove stale discussions, or clearly mark them as historical. Consider moving very large commentary to an appendix (another section tagged `:nodeclare:`) if it’s not needed for everyday editing. You can always show that to the LLM if needed.

- **Use Examples to Teach the Model:** If you want the LLM to perform a specific task (like adding a certain type of axiom), show it a prototype in the document. E.g., “See class **Organism** above, which has a complete set of annotations. Now do the same for class **Ecosystem** below.” The LLM can analogize from the example. In ELOT, it’s easy to find a “model” entry since all info is together. You might even create a section like “**Template Class (ex:Template)**” with dummy content illustrating the ideal structure, and instruct the LLM to follow that template for other classes. This is similar to one-shot or few-shot prompting but embodied in the ontology doc. (Just tag it with `:nodeclare:` so it doesn’t become a real ontology element).

- **Validate Often:** After the LLM makes changes, use ELOT’s tooling to tangle and validate the OWL (e.g., run a reasoner or check in Protégé) before accepting more changes. This is less about format and more about process, but it’s crucial. The format will help you spot issues (e.g. if the tangle fails, ELOT will pinpoint where the syntax broke). When an error is found, you can even feed the error message back to the LLM along with the snippet – since the format is textual, error messages (like “unknown prefix” or “malformed axiom at line X”) will reference the text the LLM wrote, and the LLM can help fix it. Keeping the ontology in a compilable state ensures the LLM’s contributions remain grounded in valid OWL.

- **Document AI Contributions:** For future maintainers (or your future self), note where the LLM had a hand. ELOT being a literate format means you can write things like “*Note:* The following examples were generated with assistance from ChatGPT on 2025-04-20.” This transparency helps in evaluating the ontology later. It might also be important for provenance. If an LLM suggested a relation that turned out problematic, having a note can remind you to double-check similar suggestions.

By following these practices, ontology authors can create ELOT documents that are almost *tutorial-like* for LLMs – guiding the model to be a helpful collaborator rather than a hallucination generator. The key is to play to the strengths of both ELOT and the LLM: ELOT brings organization and clarity, the LLM brings knowledge and linguistic generation. Meeting in the middle yields the best results.

## Future Outlook: Enhancing ELOT for AI-Assisted Workflows

The intersection of literate ontology engineering and AI assistants is promising, and there are several opportunities to further improve the workflow:

- **Deeper Emacs Integration:** We might see Emacs/ELOT extensions that interface directly with LLM APIs (similar to how code editors integrate Copilot). For example, an Emacs command could take the current Org subtree and send it to GPT-4 with a prompt like “suggest improvements” or “add missing elements,” then insert the result as draft text. Some community tools already move in this direction for Org-mode in general (e.g., org-ai or gptel for Emacs). Tailoring one for ontology structure – perhaps by pre-loading a prompt with ontology editing instructions – could streamline the process. This would effectively make the LLM a part of the authoring environment, on-call whenever the user needs it.

- **Automated Documentation Q&A:** ELOT could incorporate an “Ask the Ontology” feature where an LLM answers natural language questions using the ontology’s content. Since ELOT contains both formal axioms and informal explanations, an LLM could combine them to answer, for instance, “*What is the difference between a generically dependent continuant and a specifically dependent continuant?*”. The answer could cite definitions or even derive the difference from axioms. While this is more about using the ontology, it actually feeds back into authoring: if the LLM has trouble answering correctly, that might indicate the ontology or its documentation needs improvement. This could be turned into a diagnostic tool (much like competency questions, but answered by an AI to see if the ontology is sufficient).

- **Competency Questions Integration:** Building on the above, future ELOT patterns could include a section for competency questions (CQs) written in natural language. An LLM could attempt to translate these CQs into OWL axioms or SPARQL queries, or simply use them to sanity-check the ontology. For example, a CQ “Can a Person be married to more than one Person at the same time?” could prompt an LLM to check if the ontology has a restriction on the **marriedTo** property. If not, it might suggest adding a cardinality constraint. ELOT, being free-form, can hold such questions right alongside the ontology, enabling a *dialogue* between requirements and ontology content.

- **Curated Prompt Libraries:** The community could develop prompt templates specifically for ontology tasks (some exist informally, e.g., OBO Foundry has discussed ChatGPT usage for mappings and synonyms ([Leveraging ChatGPT for ontology curation - OBO Semantic Engineering Training](https://oboacademy.github.io/obook/reference/chatgpt-prompts-for-ontology-development/#:~:text=here))). These could be incorporated into ELOT’s documentation or even as Org Babel functions. Imagine having a snippet in the ELOT README like “To generate synonyms, use prompt X” and possibly an Emacs function that automates it. ELOT could ship with a guide (perhaps an `.org` file like the included `llm-prompt.org`) containing tested prompts for common ontology edits. This would lower the barrier for users to get useful output from LLMs without trial-and-error in prompt design.

- **Quality Assurance with LLMs:** We often rely on reasoners for logical consistency, but LLMs could assist with *lexical* and *consistency checks* beyond pure OWL logic. Future enhancements might include having the LLM detect inconsistent naming (e.g., some classes singular vs plural), or flag potentially deprecated concepts by comparing the ontology to current literature (via retrieval augmentation). For instance, if an ontology hasn’t been updated in a while, an LLM with access to recent knowledge could suggest new classes that are missing or point out that a term is outdated. While this extends beyond ELOT itself, the ELOT environment could serve as the staging ground for such suggestions, given it’s easy to include and review them in text form.

- **Collaborative AI Input:** Ontology development is often collaborative. One could envision a scenario where multiple human authors and an AI assistant all propose changes via pull requests on the ELOT file (since it’s under version control). To facilitate this, ELOT might incorporate metadata about who/what suggested a change. Perhaps a special annotation like `dc:contributor "GPT-4"` on axioms that came from the AI. This is speculative, but as AI contributions become commonplace, tracking provenance will be important. ELOT’s text-based nature actually makes git diffs and annotations easier (changes appear as line diffs ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=readable%20%28e,explanatory%20text%20around%20it%2C%20which)), which can be reviewed). In the future, an AI could even generate a commit message summarizing its changes (somewhat like how GitHub Copilot can suggest commit messages) – ELOT could encourage that by template.

- **Enhanced Documentation Generation:** ELOT already produces nice documentation from the literate source. A future idea is to use LLMs to *enrich* that documentation. For example, when generating the HTML docs, an LLM could be invoked to produce additional explanatory paragraphs or to create usage examples for classes that lack them (pulling from general knowledge). This would be an automated step after ontology editing, which could be reviewed by human editors. Essentially, AI could serve as a secondary documentation writer that complements the human-written parts. Since ELOT keeps documentation coupled to the ontology, any AI-written additions would be right there in the Org file for verification and tweaking.

In summary, the future of ELOT in an AI-assisted context looks bright. The tool’s philosophy of **readability and integration** positions it well to harness generative AI. As both the ontology community and LLM technology evolve, we can expect even tighter coupling – where an ontology editor is not just a passive text editor but an *interactive AI-powered assistant*. ELOT is a step in that direction, and our analysis suggests that its foundations are solid for making the most of LLM assistance.

**Conclusion:** The hypothesis stands that ELOT’s literate, Org-mode format can significantly enhance the effectiveness of LLMs in ontology authoring. By making the ontology *transparent and narrative-rich*, ELOT gives the LLM much more to work with – analogous to how a well-documented codebase enables better AI code suggestions. While using LLMs for ontology engineering is still an emerging practice, all signs indicate that a human-friendly format like ELOT lowers the communication barrier between the ontology and the AI. The result is a more productive collaboration, where tedious tasks (like writing boilerplate definitions or ensuring consistency) can be offloaded to the AI, and the human ontologist can focus on validating and guiding the model. In a field where precision and understanding are paramount, having the ontology spelled out in plain language (literally) is a huge advantage. ELOT provides that, and thus is a promising facilitator for the next generation of intelligent ontology editing tools ([GitHub - johanwk/elot: Emacs Literate Ontology Tool](https://github.com/johanwk/elot#:~:text=ELOT%20takes%20inspiration%20from%20this,HTML%20or%20PDF)) ([elot/o3-research-comparing-elot.md at main · johanwk/elot · GitHub](https://github.com/johanwk/elot/blob/main/o3-research-comparing-elot.md#:~:text=Comparatively%2C%20most%20other%20ontology%20tools,coherent%20commentary%20around%20the%20ontology)). The marriage of ELOT and ChatGPT-like models exemplifies how **structure + AI = smarter ontology development**. The critical takeaway for practitioners is that by investing in a more readable ontology format, one not only improves human collaboration but also unlocks machine assistance that was far less viable with traditional, opaque formats.
